{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45fcd7c",
   "metadata": {},
   "source": [
    "参考资料：https://blog.csdn.net/u013733326/article/details/80890454\n",
    "\n",
    "# 一步步搭建循环神经网络\n",
    "\n",
    "在这个章节中，你将使用numpy实现一个循环神经网络\n",
    "\n",
    "循环神经网络（RNN）对于自然语言处理和其他序列任务非常有效，因为它们具有“记忆”功能。 它们可以一次读取一个输入$x^{<t>}$（如单词），并且通过隐藏层激活从一个时间步传递到下一个时间步来记住一些信息/上下文，这允许单向RNN从过去获取信息来处理后面的输入，双向RNN可以从过去和未来中获取上下文\n",
    "\n",
    "有些东西需要声明：\n",
    "- 上标[l]表示第l层\n",
    "    - 举例：$a^{[4]}$表示第4层的激活值，$W^{[5]}与b^{[5]}$是第5层的参数\n",
    "- 上标(i)表示第i个样本\n",
    "    - 举例：$x^{(i)}$表示第i个输入的样本\n",
    "- 上标<t>表示第t个时间步\n",
    "    - 举例：$x^{<t>}$表示输入x的第t个时间步，$x^{(i)<t>}$表示输入x的第i个样本的第t个时间步\n",
    "- 下标i表示向量的第i项\n",
    "    - 举例：$a_i^{[l]}$表示l层中的第i个项的激活值\n",
    "    \n",
    "让我们先加载所需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db340a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e22a618",
   "metadata": {},
   "source": [
    "## 1 - 循环神经网络的前向传播\n",
    "\n",
    "我们来看一下下面的循环神经网络的图，在这里使用的是$T_x = T_y$，我们来实现它\n",
    "![](images/RNN.png)\n",
    "<center>图1：基本的RNN模型</center>\n",
    "\n",
    "我们怎么才能实现它呢?有以下几个步骤：\n",
    "1. 实现RNN的一个时间步所需要计算的东西\n",
    "2. 在$T_x$时间步上实现一个循环，以便一次处理所有的输入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1578533b",
   "metadata": {},
   "source": [
    "### 1.1 - RNN单元\n",
    "\n",
    "循环神经网络可以看作是单元的重复，首先要实现单个时间步的计算，下图描述了RNN单元的单个时间步的操作\n",
    "![](images/rnn_step_forward.png)\n",
    "<center>图2：基本的RNN单元. 输入 $x^{\\langle t \\rangle}$ (当前输入) 与 $a^{\\langle t - 1\\rangle}$ (包含过去信息的上一隐藏层的激活值), 输出 $a^{\\langle t \\rangle}$ 给下一个RNN单元，也用于预测 $y^{\\langle t \\rangle}$</center>\n",
    "\n",
    "现在我们要根据图2来实现一个RNN单元，这需要由以下几步完成：\n",
    "![](images/RNN_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8979847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    根据图2实现RNN单元的单步前向传播\n",
    "    \n",
    "    参数：\n",
    "        xt -- 时间步“t”输入的数据，维度为（n_x, m）\n",
    "        a_prev -- 时间步“t - 1”的隐藏隐藏状态，维度为（n_a, m）\n",
    "        parameters -- 字典，包含了以下内容:\n",
    "                        Wax -- 矩阵，输入乘以权重，维度为（n_a, n_x）\n",
    "                        Waa -- 矩阵，隐藏状态乘以权重，维度为（n_a, n_a）\n",
    "                        Wya -- 矩阵，隐藏状态与输出相关的权重矩阵，维度为（n_y, n_a）\n",
    "                        ba  -- 偏置，维度为（n_a, 1）\n",
    "                        by  -- 偏置，隐藏状态与输出相关的偏置，维度为（n_y, 1）\n",
    "    \n",
    "    返回：\n",
    "        a_next -- 下一个隐藏状态，维度为（n_a， m）\n",
    "        yt_pred -- 在时间步“t”的预测，维度为（n_y， m）\n",
    "        cache -- 反向传播需要的元组，包含了(a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从“parameters”获取参数\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # 使用上面的公式计算下一个激活值\n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    \n",
    "    # 使用上面的公式计算当前单元的输出\n",
    "    yt_pred = rnn_utils.softmax(np.dot(Wya, a_next) + by)\n",
    "    \n",
    "    # 保存反向传播需要的值\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d506fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape =  (5, 10)\n",
      "yt_pred[1] = [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
      " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
      "yt_pred.shape =  (2, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389242b9",
   "metadata": {},
   "source": [
    "### 1.2 - RNN的前向传播\n",
    "\n",
    "可以看到的是RNN是刚刚构建的单元格的重复连接，如果输入的数据序列经过10个时间步，那么将复制RNN单元10次，每个单元将前一个单元中的隐藏状态$(a^{<t-1>})$和当前时间步的输入数据$(x^{<t>})$作为输入。它为此时间步输出隐藏状态$(a^{<t>})$和预测$(y^{<t>})$\n",
    "![](images/rnn1.png)\n",
    "<center>图3：基本的RNN，序列 $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$输入$T_x$时间步，输出 $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$</center>\n",
    "\n",
    "我们要根据图3来实现前向传播的代码，它由以下几步构成：\n",
    "1. 创建0向量zeros(a)，它将保存RNN计算的所有的隐藏状态\n",
    "2. 使用$a_0$初始化next隐藏状态\n",
    "3. 循环所有时间同步：\n",
    "    - 使用rnn_cell_forward函数来更新“next”隐藏状态与cache\n",
    "    - 使用a来保存next隐藏状态（第t）个位置\n",
    "    - 使用y来保存预测值\n",
    "    - 把cache保存到caches列表中\n",
    "4. 返回a,y与caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187d23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    根据图3来实现循环神经网络的前向传播\n",
    "    \n",
    "    参数：\n",
    "        x -- 输入的全部数据，维度为(n_x, m, T_x)\n",
    "        a0 -- 初始化隐藏状态，维度为 (n_a, m)\n",
    "        parameters -- 字典，包含了以下内容:\n",
    "                        Wax -- 矩阵，输入乘以权重，维度为（n_a, n_x）\n",
    "                        Waa -- 矩阵，隐藏状态乘以权重，维度为（n_a, n_a）\n",
    "                        Wya -- 矩阵，隐藏状态与输出相关的权重矩阵，维度为（n_y, n_a）\n",
    "                        ba  -- 偏置，维度为（n_a, 1）\n",
    "                        by  -- 偏置，隐藏状态与输出相关的偏置，维度为（n_y, 1）\n",
    "    \n",
    "    返回：\n",
    "        a -- 所有时间步的隐藏状态，维度为(n_a, m, T_x)\n",
    "        y_pred -- 所有时间步的预测，维度为(n_y, m, T_x)\n",
    "        caches -- 为反向传播的保存的元组，维度为（【列表类型】cache, x)）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 初始化“caches”，它将以列表类型包含所有的cache\n",
    "    caches = []\n",
    "    \n",
    "    # 获取 x 与 Wya 的维度信息\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    # 使用0来初始化“a” 与“y”\n",
    "    a = np.zeros([n_a, m, T_x])\n",
    "    y_pred = np.zeros([n_y, m, T_x])\n",
    "    \n",
    "    # 初始化“next”\n",
    "    a_next = a0\n",
    "    \n",
    "    # 遍历所有时间步\n",
    "    for t in range(T_x):\n",
    "        ## 1.使用rnn_cell_forward函数来更新“next”隐藏状态与cache。\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters)\n",
    "        \n",
    "        ## 2.使用 a 来保存“next”隐藏状态（第 t ）个位置。\n",
    "        a[:, :, t] = a_next\n",
    "        \n",
    "        ## 3.使用 y 来保存预测值。\n",
    "        y_pred[:, :, t] = yt_pred\n",
    "        \n",
    "        ## 4.把cache保存到“caches”列表中。\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # 保存反向传播所需要的参数\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0793f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
      "a.shape =  (5, 10, 4)\n",
      "y_pred[1][3] = [0.79560373 0.86224861 0.11118257 0.81515947]\n",
      "y_pred.shape =  (2, 10, 4)\n",
      "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
    "print(\"a[4][1] = \", a[4][1])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
    "print(\"y_pred.shape = \", y_pred.shape)\n",
    "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64e348",
   "metadata": {},
   "source": [
    "我们构建了循环神经网络的前向传播函数，这对于某些应用程序来说已经足够好了，但是它还存在梯度消失的问题。当每个输出$y^{<t>}$是根据局部的上下文来进行预测的时候，它的效果是比较好的（意思是输入的是$x^{<t'>}$，其中t'与t相隔不是太远）\n",
    "\n",
    "接下来我们要构建一个更加复杂的LSTM模型，它可以更好地解决梯度消失的问题，LSTM能够更好地记住一条信息，并且可以在很多时间步中保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e79e39",
   "metadata": {},
   "source": [
    "## 2 - 长短时记忆（Long Short-Term Memory (LSTM)）网络\n",
    "\n",
    "下面是LSTM模块：\n",
    "![](images/LSTM.png)\n",
    "<center>图4：LSTM单元. 它跟踪与更新每个时间步上的“单元状态”或记忆变量$c^{\\langle t \\rangle}$ ，这与 $a^{\\langle t \\rangle}$不同</center>\n",
    "\n",
    "与上面的RNN例子相类似，我们先来实现一个LSTM单元，只执行一个时间步，然后在循环中调用，以处理所有输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc8a9b",
   "metadata": {},
   "source": [
    "### \"门\"的介绍\n",
    "\n",
    "#### 遗忘门\n",
    "![](images/forget.png)\n",
    "\n",
    "#### 更新门\n",
    "![](images/update.png)\n",
    "\n",
    "#### 更新单元\n",
    "![](images/update_2.png)\n",
    "\n",
    "#### 输出门\n",
    "![](images/output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836819c",
   "metadata": {},
   "source": [
    "### 2.1 - LSTM单元\n",
    "![](images/LSTM_step.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a82f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    根据图4实现一个LSTM单元的前向传播。\n",
    "    \n",
    "    参数：\n",
    "        xt -- 在时间步“t”输入的数据，维度为(n_x, m)\n",
    "        a_prev -- 上一个时间步“t-1”的隐藏状态，维度为(n_a, m)\n",
    "        c_prev -- 上一个时间步“t-1”的记忆状态，维度为(n_a, m)\n",
    "        parameters -- 字典类型的变量，包含了：\n",
    "                        Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                        Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                        Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                        bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                        Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                        Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                        by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "    返回：\n",
    "        a_next -- 下一个隐藏状态，维度为(n_a, m)\n",
    "        c_next -- 下一个记忆状态，维度为(n_a, m)\n",
    "        yt_pred -- 在时间步“t”的预测，维度为(n_y, m)\n",
    "        cache -- 包含了反向传播所需要的参数，包含了(a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "        \n",
    "    注意：\n",
    "        ft/it/ot表示遗忘/更新/输出门，cct表示候选值(c tilda)，c表示记忆值。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从“parameters”中获取相关值\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # 获取 xt 与 Wy 的维度信息\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    \n",
    "    # 1.连接 a_prev 与 xt\n",
    "    contact = np.zeros([n_a + n_x, m])\n",
    "    contact[: n_a, :] = a_prev\n",
    "    contact[n_a :, :] = xt\n",
    "    \n",
    "    # 2.根据公式计算ft、it、cct、c_next、ot、a_next\n",
    "    \n",
    "    ## 遗忘门，公式1\n",
    "    ft = rnn_utils.sigmoid(np.dot(Wf, contact) + bf)\n",
    "    \n",
    "    ## 更新门，公式2\n",
    "    it = rnn_utils.sigmoid(np.dot(Wi, contact) + bi)\n",
    "    \n",
    "    ## 更新单元，公式3\n",
    "    cct = np.tanh(np.dot(Wc, contact) + bc)\n",
    "    \n",
    "    ## 更新单元，公式4\n",
    "    #c_next = np.multiply(ft, c_prev) + np.multiply(it, cct)\n",
    "    c_next = ft * c_prev + it * cct\n",
    "    ## 输出门，公式5\n",
    "    ot = rnn_utils.sigmoid(np.dot(Wo, contact) + bo)\n",
    "    \n",
    "    ## 输出门，公式6\n",
    "    #a_next = np.multiply(ot, np.tan(c_next))\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    # 3.计算LSTM单元的预测值\n",
    "    yt_pred = rnn_utils.softmax(np.dot(Wy, a_next) + by)\n",
    "    \n",
    "    # 保存包含了反向传播所需要的参数\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "    \n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026dab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
      "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
      "a_next.shape =  (5, 10)\n",
      "c_next[2] =  [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
      "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
      "c_next.shape =  (5, 10)\n",
      "yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n",
      " 0.00943007 0.12666353 0.39380172 0.07828381]\n",
      "yt.shape =  (2, 10)\n",
      "cache[1][3] = [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
      "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
      "len(cache) =  10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", c_next.shape)\n",
    "print(\"c_next[2] = \", c_next[2])\n",
    "print(\"c_next.shape = \", c_next.shape)\n",
    "print(\"yt[1] =\", yt[1])\n",
    "print(\"yt.shape = \", yt.shape)\n",
    "print(\"cache[1][3] =\", cache[1][3])\n",
    "print(\"len(cache) = \", len(cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff5aca",
   "metadata": {},
   "source": [
    "### 2.2 - LSTM的前向传播\n",
    "\n",
    "我们已经实现了LSTM单元的一个时间步的前向传播，现在我们要对LSTM网络进行前向传播进行计算\n",
    "![](images/LSTM_rnn.png)\n",
    "<center>图5：所有时间步的LSTM的计算</center>\n",
    "\n",
    "我们来实现lstm_forward()，然后运行$T_x$个时间步\n",
    "\n",
    "注意：$c^{<0>}$使用0来初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0543bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    根据图5来实现LSTM单元组成的的循环神经网络\n",
    "    \n",
    "    参数：\n",
    "        x -- 所有时间步的输入数据，维度为(n_x, m, T_x)\n",
    "        a0 -- 初始化隐藏状态，维度为(n_a, m)\n",
    "        parameters -- python字典，包含了以下参数：\n",
    "                        Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                        Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                        Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                        bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                        Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                        Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                        by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "        \n",
    "    返回：\n",
    "        a -- 所有时间步的隐藏状态，维度为(n_a, m, T_x)\n",
    "        y -- 所有时间步的预测值，维度为(n_y, m, T_x)\n",
    "        caches -- 为反向传播的保存的元组，维度为（【列表类型】cache, x)）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 初始化“caches”\n",
    "    caches = []\n",
    "    \n",
    "    # 获取 xt 与 Wy 的维度信息\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wy\"].shape\n",
    "    \n",
    "    # 使用0来初始化“a”、“c”、“y”\n",
    "    a = np.zeros([n_a, m, T_x])\n",
    "    c = np.zeros([n_a, m, T_x])\n",
    "    y = np.zeros([n_y, m, T_x])\n",
    "    \n",
    "    # 初始化“a_next”、“c_next”\n",
    "    a_next = a0\n",
    "    c_next = np.zeros([n_a, m])\n",
    "    \n",
    "    # 遍历所有的时间步\n",
    "    for t in range(T_x):\n",
    "        # 更新下一个隐藏状态，下一个记忆状态，计算预测值，获取cache\n",
    "        a_next, c_next, yt_pred, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)\n",
    "        \n",
    "        # 保存新的下一个隐藏状态到变量a中\n",
    "        a[:, :, t] = a_next\n",
    "        \n",
    "        # 保存预测值到变量y中\n",
    "        y[:, :, t] = yt_pred\n",
    "        \n",
    "        # 保存下一个单元状态到变量c中\n",
    "        c[:, :, t] = c_next\n",
    "        \n",
    "        # 把cache添加到caches中\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # 保存反向传播需要的参数\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c685b342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][3][6] =  0.17211776753291672\n",
      "a.shape =  (5, 10, 7)\n",
      "y[1][4][3] = 0.9508734618501101\n",
      "y.shape =  (2, 10, 7)\n",
      "caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
      "  0.41005165]\n",
      "c[1][2][1] -0.8555449167181981\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "print(\"a[4][3][6] = \", a[4][3][6])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y[1][4][3] =\", y[1][4][3])\n",
    "print(\"y.shape = \", y.shape)\n",
    "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
    "print(\"c[1][2][1]\", c[1][2][1])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aaae15",
   "metadata": {},
   "source": [
    "## 3 - 循环神经网络的反向传播（选学）\n",
    "\n",
    "现代深度学习框架中，我们只需要实现前向传播，框架负责反向传播，因此大多数深度学习工程师不需要为反向传播的细节而烦恼。 但是，如果您是微积分方面的专家并希望在RNN中查看反向传播的详细信息，则可以学习这个选学部分\n",
    "\n",
    "在前面课程中，我们实现了一个简单（完全连接）的神经网络，我们使用反向传播来计算与更新参数的成本相关的导数。类似地，在循环神经网络中，我们可以计算与成本相关的导数，以便更新参数。反向传播的方程非常复杂，我们没有在视频中推导它们，但是，我们将在下面简要介绍它们"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c562bf",
   "metadata": {},
   "source": [
    "### 3.1 - 基本的RNN网络的反向传播\n",
    "\n",
    "我们将开始计算基本的RNN单元的反向传播，我们先来看一下下面的图：\n",
    "![](images/rnn_cell_backprop.png)\n",
    "<center>图6：RNN单元的反向传播。就像在全连接的神经网络中，代价函数$J$的导数通过遵循链式规则从RNN进行反向传播，链式法则也用于计算$(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$来更新参数$(W_{ax}, W_{aa}, b_a)$</center>\n",
    "\n",
    "单向传播的推导：为了计算rnn_cell_backward，我们需要计算下面的公式：\n",
    "![](images/other_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7083ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    实现基本的RNN单元的单步反向传播\n",
    "    \n",
    "    参数：\n",
    "        da_next -- 关于下一个隐藏状态的损失的梯度。\n",
    "        cache -- 字典类型，rnn_step_forward()的输出\n",
    "        \n",
    "    返回：\n",
    "        gradients -- 字典，包含了以下参数：\n",
    "                        dx -- 输入数据的梯度，维度为(n_x, m)\n",
    "                        da_prev -- 上一隐藏层的隐藏状态，维度为(n_a, m)\n",
    "                        dWax -- 输入到隐藏状态的权重的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 隐藏状态到隐藏状态的权重的梯度，维度为(n_a, n_a)\n",
    "                        dba -- 偏置向量的梯度，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # 获取cache 的值\n",
    "    a_next, a_prev, xt, parameters = cache\n",
    "    \n",
    "    # 从 parameters 中获取参数\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # 计算tanh相对于a_next的梯度.\n",
    "    dtanh = (1 - np.square(a_next)) * da_next\n",
    "    \n",
    "    # 计算关于Wax损失的梯度\n",
    "    dxt = np.dot(Wax.T,dtanh)\n",
    "    dWax = np.dot(dtanh, xt.T)\n",
    "    \n",
    "    # 计算关于Waa损失的梯度\n",
    "    da_prev = np.dot(Waa.T,dtanh)\n",
    "    dWaa = np.dot(dtanh, a_prev.T)\n",
    "    \n",
    "    # 计算关于b损失的梯度\n",
    "    dba = np.sum(dtanh, keepdims=True, axis=-1)\n",
    "    \n",
    "    # 保存这些梯度到字典内\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccffacc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dxt\"][1][2] = -0.4605641030588796\n",
      "gradients[\"dxt\"].shape = (3, 10)\n",
      "gradients[\"da_prev\"][2][3] = 0.08429686538067718\n",
      "gradients[\"da_prev\"].shape = (5, 10)\n",
      "gradients[\"dWax\"][3][1] = 0.3930818739219303\n",
      "gradients[\"dWax\"].shape = (5, 3)\n",
      "gradients[\"dWaa\"][1][2] = -0.2848395578696067\n",
      "gradients[\"dWaa\"].shape = (5, 5)\n",
      "gradients[\"dba\"][4] = [0.80517166]\n",
      "gradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "b = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(5,10)\n",
    "gradients = rnn_cell_backward(da_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d310806",
   "metadata": {},
   "source": [
    "单步反向传播已经实现了，我们接下来就实现整个循环神经网络的反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf09fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    \"\"\"\n",
    "    在整个输入数据序列上实现RNN的反向传播\n",
    "    \n",
    "    参数：\n",
    "        da -- 所有隐藏状态的梯度，维度为(n_a, m, T_x)\n",
    "        caches -- 包含向前传播的信息的元组\n",
    "    \n",
    "    返回：    \n",
    "        gradients -- 包含了梯度的字典：\n",
    "                        dx -- 关于输入数据的梯度，维度为(n_x, m, T_x)\n",
    "                        da0 -- 关于初始化隐藏状态的梯度，维度为(n_a, m)\n",
    "                        dWax -- 关于输入权重的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 关于隐藏状态的权值的梯度，维度为(n_a, n_a)\n",
    "                        dba -- 关于偏置的梯度，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # 从caches中获取第一个cache（t=1）的值\n",
    "    caches, x = caches\n",
    "    a1, a0, x1, parameters = caches[0]\n",
    "    \n",
    "    # 获取da与x1的维度信息\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # 初始化梯度\n",
    "    dx = np.zeros([n_x, m, T_x])\n",
    "    dWax = np.zeros([n_a, n_x])\n",
    "    dWaa = np.zeros([n_a, n_a])\n",
    "    dba = np.zeros([n_a, 1])\n",
    "    da0 = np.zeros([n_a, m])\n",
    "    da_prevt = np.zeros([n_a, m])\n",
    "    \n",
    "    # 处理所有时间步\n",
    "    for t in reversed(range(T_x)):\n",
    "        # 计算时间步“t”时的梯度\n",
    "        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])\n",
    "        \n",
    "        #从梯度中获取导数\n",
    "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
    "        \n",
    "        # 通过在时间步t添加它们的导数来增加关于全局导数的参数\n",
    "        dx[:, :, t] = dxt\n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba += dbat\n",
    "        \n",
    "    #将 da0设置为a的梯度，该梯度已通过所有时间步骤进行反向传播\n",
    "    da0 = da_prevt\n",
    "    \n",
    "    #保存这些梯度到字典内\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4a81dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
      "gradients[\"dx\"].shape = (3, 10, 4)\n",
      "gradients[\"da0\"][2][3] = -0.31494237512664996\n",
      "gradients[\"da0\"].shape = (5, 10)\n",
      "gradients[\"dWax\"][3][1] = 11.264104496527777\n",
      "gradients[\"dWax\"].shape = (5, 3)\n",
      "gradients[\"dWaa\"][1][2] = 2.303333126579893\n",
      "gradients[\"dWaa\"].shape = (5, 5)\n",
      "gradients[\"dba\"][4] = [-0.74747722]\n",
      "gradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a, y, caches = rnn_forward(x, a0, parameters)\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = rnn_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707b7f0",
   "metadata": {},
   "source": [
    "### 3.2 - LSTM反向传播\n",
    "\n",
    "#### 3.2.1 - 单步反向传播\n",
    "LSTM反向传播比前向传播更复杂一些。我们已经提供了下面LSTM反向传播的所有方程。(如果你喜欢微积分的练习，你可以自己尝试去推导这些\n",
    "\n",
    "#### 3.2.2 门的导数\n",
    "![](images/other_2.png)\n",
    "\n",
    "#### 3.2.3 参数的导数\n",
    "![](images/other_3.png)\n",
    "![](imahes/other_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1938f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    实现LSTM的单步反向传播\n",
    "    \n",
    "    参数：\n",
    "        da_next -- 下一个隐藏状态的梯度，维度为(n_a, m)\n",
    "        dc_next -- 下一个单元状态的梯度，维度为(n_a, m)\n",
    "        cache -- 来自前向传播的一些参数\n",
    "        \n",
    "    返回：\n",
    "        gradients -- 包含了梯度信息的字典：\n",
    "                        dxt -- 输入数据的梯度，维度为(n_x, m)\n",
    "                        da_prev -- 先前的隐藏状态的梯度，维度为(n_a, m)\n",
    "                        dc_prev -- 前的记忆状态的梯度，维度为(n_a, m, T_x)\n",
    "                        dWf -- 遗忘门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbf -- 遗忘门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWi -- 更新门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbi -- 更新门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWc -- 第一个“tanh”的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbc -- 第一个“tanh”的偏置的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dWo -- 输出门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbo -- 输出门的偏置的梯度，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # 从cache中获取信息\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    \n",
    "    # 获取xt与a_next的维度信息\n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = a_next.shape\n",
    "    \n",
    "    # 根据公式7-10来计算门的导数\n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = (dc_next * it + ot * (1 - np.square(np.tanh(c_next))) * it * da_next) * (1 - np.square(cct))\n",
    "    dit = (dc_next * cct + ot * (1 - np.square(np.tanh(c_next))) * cct * da_next) * it * (1 - it)\n",
    "    dft = (dc_next * c_prev + ot * (1 - np.square(np.tanh(c_next))) * c_prev * da_next) * ft * (1 - ft)\n",
    "    \n",
    "    # 根据公式11-14计算参数的导数\n",
    "    concat = np.concatenate((a_prev, xt), axis=0).T\n",
    "    dWf = np.dot(dft, concat)\n",
    "    dWi = np.dot(dit, concat)\n",
    "    dWc = np.dot(dcct, concat)\n",
    "    dWo = np.dot(dot, concat)\n",
    "    dbf = np.sum(dft,axis=1,keepdims=True)\n",
    "    dbi = np.sum(dit,axis=1,keepdims=True)\n",
    "    dbc = np.sum(dcct,axis=1,keepdims=True)\n",
    "    dbo = np.sum(dot,axis=1,keepdims=True)\n",
    "    \n",
    "    \n",
    "    # 使用公式15-17计算洗起来了隐藏状态、先前记忆状态、输入的导数。\n",
    "    da_prev = np.dot(parameters[\"Wf\"][:, :n_a].T, dft) + np.dot(parameters[\"Wc\"][:, :n_a].T, dcct) +  np.dot(parameters[\"Wi\"][:, :n_a].T, dit) + np.dot(parameters[\"Wo\"][:, :n_a].T, dot)\n",
    "        \n",
    "    dc_prev = dc_next * ft + ot * (1 - np.square(np.tanh(c_next))) * ft * da_next\n",
    "    \n",
    "    dxt = np.dot(parameters[\"Wf\"][:, n_a:].T, dft) + np.dot(parameters[\"Wc\"][:, n_a:].T, dcct) +  np.dot(parameters[\"Wi\"][:, n_a:].T, dit) + np.dot(parameters[\"Wo\"][:, n_a:].T, dot)\n",
    "    \n",
    "    # 保存梯度信息到字典\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84041fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dxt\"][1][2] = 3.2305591151091884\n",
      "gradients[\"dxt\"].shape = (3, 10)\n",
      "gradients[\"da_prev\"][2][3] = -0.06396214197109241\n",
      "gradients[\"da_prev\"].shape = (5, 10)\n",
      "gradients[\"dc_prev\"][2][3] = 0.7975220387970015\n",
      "gradients[\"dc_prev\"].shape = (5, 10)\n",
      "gradients[\"dWf\"][3][1] = -0.1479548381644968\n",
      "gradients[\"dWf\"].shape = (5, 8)\n",
      "gradients[\"dWi\"][1][2] = 1.0574980552259903\n",
      "gradients[\"dWi\"].shape = (5, 8)\n",
      "gradients[\"dWc\"][3][1] = 2.3045621636876668\n",
      "gradients[\"dWc\"].shape = (5, 8)\n",
      "gradients[\"dWo\"][1][2] = 0.3313115952892109\n",
      "gradients[\"dWo\"].shape = (5, 8)\n",
      "gradients[\"dbf\"][4] = [0.18864637]\n",
      "gradients[\"dbf\"].shape = (5, 1)\n",
      "gradients[\"dbi\"][4] = [-0.40142491]\n",
      "gradients[\"dbi\"].shape = (5, 1)\n",
      "gradients[\"dbc\"][4] = [0.25587763]\n",
      "gradients[\"dbc\"].shape = (5, 1)\n",
      "gradients[\"dbo\"][4] = [0.13893342]\n",
      "gradients[\"dbo\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(5,10)\n",
    "dc_next = np.random.randn(5,10)\n",
    "gradients = lstm_cell_backward(da_next, dc_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dc_prev\\\"][2][3] =\", gradients[\"dc_prev\"][2][3])\n",
    "print(\"gradients[\\\"dc_prev\\\"].shape =\", gradients[\"dc_prev\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b9417",
   "metadata": {},
   "source": [
    "### 3.3 - LSTM网络的反向传播\n",
    "\n",
    "这部分与我们在上面实现的rnn_backward函数非常相似。我们将首先创建与返回变量相同维度的变量。然后将遍历从结束到开始的所有时间步，并调用在每次迭代时为LSTM实现的单步反向传播功能。然后我们将通过单独求和来更新参数，最后返回一个带有新梯度的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba6e2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    实现LSTM网络的反向传播\n",
    "    \n",
    "    参数：\n",
    "        da -- 关于隐藏状态的梯度，维度为(n_a, m, T_x)\n",
    "        cachses -- 前向传播保存的信息\n",
    "    \n",
    "    返回：\n",
    "        gradients -- 包含了梯度信息的字典：\n",
    "                        dx -- 输入数据的梯度，维度为(n_x, m，T_x)\n",
    "                        da0 -- 先前的隐藏状态的梯度，维度为(n_a, m)\n",
    "                        dWf -- 遗忘门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbf -- 遗忘门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWi -- 更新门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbi -- 更新门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWc -- 第一个“tanh”的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbc -- 第一个“tanh”的偏置的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dWo -- 输出门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbo -- 输出门的偏置的梯度，维度为(n_a, 1)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # 从caches中获取第一个cache（t=1）的值\n",
    "    caches, x = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    # 获取da与x1的维度信息\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # 初始化梯度\n",
    "    dx = np.zeros([n_x, m, T_x])\n",
    "    da0 = np.zeros([n_a, m])\n",
    "    da_prevt = np.zeros([n_a, m])\n",
    "    dc_prevt = np.zeros([n_a, m])\n",
    "    dWf = np.zeros([n_a, n_a + n_x])\n",
    "    dWi = np.zeros([n_a, n_a + n_x])\n",
    "    dWc = np.zeros([n_a, n_a + n_x])\n",
    "    dWo = np.zeros([n_a, n_a + n_x])\n",
    "    dbf = np.zeros([n_a, 1])\n",
    "    dbi = np.zeros([n_a, 1])\n",
    "    dbc = np.zeros([n_a, 1])\n",
    "    dbo = np.zeros([n_a, 1])\n",
    "    \n",
    "    # 处理所有时间步\n",
    "    for t in reversed(range(T_x)):\n",
    "        # 使用lstm_cell_backward函数计算所有梯度\n",
    "        gradients = lstm_cell_backward(da[:,:,t],dc_prevt,caches[t])\n",
    "        # 保存相关参数\n",
    "        dx[:,:,t] = gradients['dxt']\n",
    "        dWf = dWf+gradients['dWf']\n",
    "        dWi = dWi+gradients['dWi']\n",
    "        dWc = dWc+gradients['dWc']\n",
    "        dWo = dWo+gradients['dWo']\n",
    "        dbf = dbf+gradients['dbf']\n",
    "        dbi = dbi+gradients['dbi']\n",
    "        dbc = dbc+gradients['dbc']\n",
    "        dbo = dbo+gradients['dbo']\n",
    "    # 将第一个激活的梯度设置为反向传播的梯度da_prev。\n",
    "    da0 = gradients['da_prev']\n",
    "\n",
    "    # 保存所有梯度到字典变量内\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ee64f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-0.00173313  0.08287442 -0.30545663 -0.43281115]\n",
      "gradients[\"dx\"].shape = (3, 10, 4)\n",
      "gradients[\"da0\"][2][3] = -0.09591150195400465\n",
      "gradients[\"da0\"].shape = (5, 10)\n",
      "gradients[\"dWf\"][3][1] = -0.06981985612744009\n",
      "gradients[\"dWf\"].shape = (5, 8)\n",
      "gradients[\"dWi\"][1][2] = 0.10237182024854771\n",
      "gradients[\"dWi\"].shape = (5, 8)\n",
      "gradients[\"dWc\"][3][1] = -0.062498379492745226\n",
      "gradients[\"dWc\"].shape = (5, 8)\n",
      "gradients[\"dWo\"][1][2] = 0.04843891314443013\n",
      "gradients[\"dWo\"].shape = (5, 8)\n",
      "gradients[\"dbf\"][4] = [-0.0565788]\n",
      "gradients[\"dbf\"].shape = (5, 1)\n",
      "gradients[\"dbi\"][4] = [-0.15399065]\n",
      "gradients[\"dbi\"].shape = (5, 1)\n",
      "gradients[\"dbc\"][4] = [-0.29691142]\n",
      "gradients[\"dbc\"].shape = (5, 1)\n",
      "gradients[\"dbo\"][4] = [-0.29798344]\n",
      "gradients[\"dbo\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = lstm_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e7a11",
   "metadata": {},
   "source": [
    "最基础的东西算是搭建好了，现在我们进行下一部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961abbf",
   "metadata": {},
   "source": [
    "# 字符级语言模型 - 恐龙岛\n",
    "\n",
    "欢迎来到恐龙岛，恐龙生活于在6500万年前，现在研究人员在试着复活恐龙，而你的任务就是给恐龙命名，如果一只恐龙不喜欢它的名字，它可能会狂躁不安，所以你要谨慎选择\n",
    "![](images/dino.jpg)\n",
    "你的助手已经收集了他们能够找到的所有恐龙名字，并编入了这个数据集,为了构建字符级语言模型来生成新的名称，你的模型将学习不同的名称模式，并随机生成新的名字。希望这个算法能让你和你的团队远离恐龙的愤怒\n",
    "\n",
    "在这里你将学习到：\n",
    "- 如何存储文本数据以便使用RNN进行处理\n",
    "- 如何合成数据，通过每次采样预测，并将其传递给下一个rnn单元\n",
    "- 如何构建字符级文本生成循环神经网络\n",
    "- 为什么梯度修剪很重要?\n",
    "\n",
    "我们将首先加载我们在rnn_utils中提供的一些函数。具体地说，我们可以使用rnn_forward和rnn_backward等函数，这些函数与前面实现的函数相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7f4469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import cllm_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2c5d3",
   "metadata": {},
   "source": [
    "## 1 - 问题描述\n",
    "\n",
    "### 1.1 - 数据集与预处理\n",
    "\n",
    "我们先来读取恐龙名称的数据集，创建一个唯一字符列表（如AZ），并计算数据集和词汇量大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d2f74ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'o', 'c', 'a', 't', 'h', 'k', 'y', 'b', 'i', 'n', 'p', 'e', 'q', '\\n', 'u', 's', 'r', 'j', 'f', 'w', 'v', 'l', 'm', 'g', 'x', 'z']\n",
      "共计有19909个字符，唯一字符有27个\n"
     ]
    }
   ],
   "source": [
    "# 获取名称\n",
    "data = open(\"dinos.txt\", \"r\").read()\n",
    "\n",
    "# 转化为小写字符\n",
    "data = data.lower()\n",
    "\n",
    "# 转化为无序且不重复的元素列表\n",
    "chars = list(set(data))\n",
    "\n",
    "# 获取大小信息\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print(chars)\n",
    "print(\"共计有%d个字符，唯一字符有%d个\"%(data_size,vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c8922",
   "metadata": {},
   "source": [
    "这些字符是a-z（26个英文字符）加上“\\n”（换行字符），在这里换行字符起到了在视频中类似的EOS（句子结尾）的作用，这里表示了名字的结束而不是句子的结尾。下面我们将创建一个字典，每个字符映射到0-26的索引，然后再创建一个字典，它将该字典将每个索引映射回相应的字符字符，它会帮助我们找出softmax层的概率分布输出中的字符。我们来创建char_to_ix 与 ix_to_char字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c88302cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch:i for i, ch in enumerate(sorted(chars))}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(sorted(chars))}\n",
    "\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce703a2",
   "metadata": {},
   "source": [
    "### 1.2 - 模型回顾\n",
    "\n",
    "模型结构如下：\n",
    "- 初始化参数\n",
    "- 循环：\n",
    "    - 前向传播计算损失\n",
    "    - 反向传播计算关于损失的梯度\n",
    "    - 修建梯度以免梯度爆炸\n",
    "    - 用梯度下降更新规则更新参数\n",
    "- 返回学习后的参数\n",
    "\n",
    "![](images/other_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b39b6b",
   "metadata": {},
   "source": [
    "## 2 - 构建模型中的模块\n",
    "\n",
    "在这部分，我们将来构建整个模型中的两个重要模块：\n",
    "- 梯度修剪：避免梯度爆炸\n",
    "- 取样：一种用来生产字符的技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bff2f",
   "metadata": {},
   "source": [
    "### 2.1 梯度修剪\n",
    "\n",
    "在这里，我们将实现在优化循环中调用的clip函数。回想一下，整个循环结构通常包括前向传播、成本计算、反向传播和参数更新。在更新参数之前，我们将在需要时执行梯度修剪，以确保我们的梯度不是“爆炸”的\n",
    "\n",
    "接下来我们将实现一个修剪函数，该函数输入一个梯度字典输出一个已经修剪过了的梯度。有很多的方法来修剪梯度，我们在这里使用一个比较简单的方法。梯度向量的每一个元素都被限制在[-N，N]的范围，通俗的说，有一个maxValue（比如10），如果梯度的任何值大于10，那么它将被设置为10，如果梯度的任何值小于-10，那么它将被设置为-10，如果它在-10与10之间，那么它将不变\n",
    "\n",
    "![](images/clip.png)\n",
    "<center>图2：在网络进入轻微的“梯度爆炸”的问题的情况下，使用梯度修剪和无梯度修剪的可视化图</center>\n",
    "\n",
    "我们来实现下面的函数来返回一个修剪过后的梯度字典，函数接受最大阈值，并返回修剪后的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f316bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    \"\"\"\n",
    "    使用maxValue来修剪梯度\n",
    "    \n",
    "    参数：\n",
    "        gradients -- 字典类型，包含了以下参数：\"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "        maxValue -- 阈值，把梯度值限制在[-maxValue, maxValue]内\n",
    "        \n",
    "    返回：\n",
    "        gradients -- 修剪后的梯度\n",
    "    \"\"\"\n",
    "    # 获取参数\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "    \n",
    "    # 梯度修剪\n",
    "    for gradient in [dWaa, dWax, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e08ba06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717aef6",
   "metadata": {},
   "source": [
    "### 2.2 - 采样\n",
    "\n",
    "现在假设我们的模型已经训练过了，我们希望生成新的文本，生成的过程如下图：\n",
    "![](images/dinos3.png)\n",
    "<center>图3：在这幅图中，我们假设模型已经经过了训练。我们在第一步传入$x^{\\langle 1\\rangle} = \\vec{0}$，然后让网络一次对一个字符进行采样</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487ef9f",
   "metadata": {},
   "source": [
    "![](images/other_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf02d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "p = np.array([0.1, 0.0, 0.7, 0.2])\n",
    "index = np.random.choice([0, 1, 2, 3], p = p.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ec3a7",
   "metadata": {},
   "source": [
    "![](images/other_7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdb3012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_is, seed):\n",
    "    \"\"\"\n",
    "    根据RNN输出的概率分布序列对字符序列进行采样\n",
    "    \n",
    "    参数：\n",
    "        parameters -- 包含了Waa, Wax, Wya, by, b的字典\n",
    "        char_to_ix -- 字符映射到索引的字典\n",
    "        seed -- 随机种子\n",
    "        \n",
    "    返回：\n",
    "        indices -- 包含采样字符索引的长度为n的列表。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从parameters 中获取参数\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # 步骤1 \n",
    "    ## 创建独热向量x\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    \n",
    "    ## 使用0初始化a_prev\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    # 创建索引的空列表，这是包含要生成的字符的索引的列表。\n",
    "    indices = []\n",
    "    \n",
    "    # IDX是检测换行符的标志，我们将其初始化为-1。\n",
    "    idx = -1\n",
    "    \n",
    "    # 循环遍历时间步骤t。在每个时间步中，从概率分布中抽取一个字符，\n",
    "    # 并将其索引附加到“indices”上，如果我们达到50个字符，\n",
    "    #（我们应该不太可能有一个训练好的模型），我们将停止循环，这有助于调试并防止进入无限循环\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix[\"\\n\"]\n",
    "    \n",
    "    while (idx != newline_character and counter < 50):\n",
    "        # 步骤2：使用公式1、2、3进行前向传播\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = cllm_utils.softmax(z)\n",
    "        \n",
    "        # 设定随机种子\n",
    "        np.random.seed(counter + seed)\n",
    "        \n",
    "        # 步骤3：从概率分布y中抽取词汇表中字符的索引\n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "        \n",
    "        # 添加到索引中\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # 步骤4:将输入字符重写为与采样索引对应的字符。\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # 更新a_prev为a\n",
    "        a_prev = a \n",
    "        \n",
    "        # 累加器\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "    \n",
    "    if(counter == 50):\n",
    "        indices.append(char_to_ix[\"\\n\"])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "540b2f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 12, 3, 1, 0]\n",
      "list of sampled characters: ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'l', 'c', 'a', '\\n']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "_, n_a = 20, 100\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be0bd0",
   "metadata": {},
   "source": [
    "## 3 - 构建语言模型\n",
    "\n",
    "### 3.1 - 梯度下降\n",
    "\n",
    "在这里，我们将实现一个执行随机梯度下降的一个步骤的函数（带有梯度修剪）。我们将一次训练一个样本，所以优化算法将是随机梯度下降，这里是RNN的一个通用的优化循环的步骤：\n",
    "- 前向传播计算损失\n",
    "- 反向传播计算关于参数的梯度损失\n",
    "- 修剪梯度\n",
    "- 使用梯度下降更新参数\n",
    "\n",
    "我们来实现这一优化过程（单步随机梯度下降），这里我们提供了一些函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6355f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例，请勿执行。\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    通过RNN进行前向传播，计算交叉熵损失。\n",
    "\n",
    "    它返回损失的值以及存储在反向传播中使用的“缓存”值。\n",
    "    \"\"\"\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" \n",
    "    通过时间进行反向传播，计算相对于参数的梯度损失。它还返回所有隐藏的状态\n",
    "    \"\"\"\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the Gradient Descent Update Rule\n",
    "    \"\"\"\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f87a43a",
   "metadata": {},
   "source": [
    "我们来构建优化函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12d1a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    执行训练模型的单步优化。\n",
    "    \n",
    "    参数：\n",
    "        X -- 整数列表，其中每个整数映射到词汇表中的字符。\n",
    "        Y -- 整数列表，与X完全相同，但向左移动了一个索引。\n",
    "        a_prev -- 上一个隐藏状态\n",
    "        parameters -- 字典，包含了以下参数：\n",
    "                        Wax -- 权重矩阵乘以输入，维度为(n_a, n_x)\n",
    "                        Waa -- 权重矩阵乘以隐藏状态，维度为(n_a, n_a)\n",
    "                        Wya -- 隐藏状态与输出相关的权重矩阵，维度为(n_y, n_a)\n",
    "                        b -- 偏置，维度为(n_a, 1)\n",
    "                        by -- 隐藏状态与输出相关的权重偏置，维度为(n_y, 1)\n",
    "        learning_rate -- 模型学习的速率\n",
    "    \n",
    "    返回：\n",
    "        loss -- 损失函数的值（交叉熵损失）\n",
    "        gradients -- 字典，包含了以下参数：\n",
    "                        dWax -- 输入到隐藏的权值的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 隐藏到隐藏的权值的梯度，维度为(n_a, n_a)\n",
    "                        dWya -- 隐藏到输出的权值的梯度，维度为(n_y, n_a)\n",
    "                        db -- 偏置的梯度，维度为(n_a, 1)\n",
    "                        dby -- 输出偏置向量的梯度，维度为(n_y, 1)\n",
    "        a[len(X)-1] -- 最后的隐藏状态，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 前向传播\n",
    "    loss, cache = cllm_utils.rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # 反向传播\n",
    "    gradients, a = cllm_utils.rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # 梯度修剪，[-5 , 5]\n",
    "    gradients = clip(gradients,5)\n",
    "    \n",
    "    # 更新参数\n",
    "    parameters = cllm_utils.update_parameters(parameters,gradients,learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46ad78df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 126.50397572165345\n",
      "gradients[\"dWaa\"][1][2] = 0.19470931534725341\n",
      "np.argmax(gradients[\"dWax\"]) = 93\n",
      "gradients[\"dWya\"][1][2] = -0.007773876032004315\n",
      "gradients[\"db\"][4] = [-0.06809825]\n",
      "gradients[\"dby\"][1] = [0.01538192]\n",
      "a_last[4] = [-1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size, n_a = 27, 100\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "X = [12,3,5,11,22,3]\n",
    "Y = [4,14,11,22,25, 26]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecb030",
   "metadata": {},
   "source": [
    "### 3.2 - 训练模型\n",
    "\n",
    "给定恐龙名称的数据集，我们使用数据集的每一行(一个名称)作为一个训练样本。每100步随机梯度下降，你将抽样10个随机选择的名字，看看算法是怎么做的。记住要打乱数据集，以便随机梯度下降以随机顺序访问样本。当examples[index]包含一个恐龙名称（String）时，为了创建一个样本（X,Y），你可以使用这个：\n",
    "```\n",
    "index = j % len(examples)\n",
    "X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "```\n",
    "\n",
    "![](images/other_8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f23a7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, ix_to_char, char_to_ix, num_iterations=3500, \n",
    "          n_a=50, dino_names=7,vocab_size=27):\n",
    "    \"\"\"\n",
    "    训练模型并生成恐龙名字\n",
    "    \n",
    "    参数：\n",
    "        data -- 语料库\n",
    "        ix_to_char -- 索引映射字符字典\n",
    "        char_to_ix -- 字符映射索引字典\n",
    "        num_iterations -- 迭代次数\n",
    "        n_a -- RNN单元数量\n",
    "        dino_names -- 每次迭代中采样的数量\n",
    "        vocab_size -- 在文本中的唯一字符的数量\n",
    "    \n",
    "    返回：\n",
    "        parameters -- 学习后了的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从vocab_size中获取n_x、n_y\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # 初始化参数\n",
    "    parameters = cllm_utils.initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # 初始化损失\n",
    "    loss = cllm_utils.get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # 构建恐龙名称列表\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # 打乱全部的恐龙名称\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # 初始化LSTM隐藏状态\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    # 循环\n",
    "    for j in range(num_iterations):\n",
    "        # 定义一个训练样本\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        # 执行单步优化：前向传播 -> 反向传播 -> 梯度修剪 -> 更新参数\n",
    "        # 选择学习率为0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        # 使用延迟来保持损失平滑,这是为了加速训练。\n",
    "        loss = cllm_utils.smooth(loss, curr_loss)\n",
    "        \n",
    "        # 每2000次迭代，通过sample()生成“\\n”字符，检查模型是否学习正确\n",
    "        if j % 2000 == 0:\n",
    "            print(\"第\" + str(j+1) + \"次迭代，损失值为：\" + str(loss))\n",
    "            \n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                # 采样\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                cllm_utils.print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                # 为了得到相同的效果，随机种子+1\n",
    "                seed += 1\n",
    "            \n",
    "            print(\"\\n\")\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04c7696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次迭代，损失值为：23.087336085484605\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "第2001次迭代，损失值为：27.884160491415773\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "执行了：0分4秒\n"
     ]
    }
   ],
   "source": [
    "#开始时间\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "#开始训练\n",
    "parameters = model(data, ix_to_char, char_to_ix, num_iterations=3500)\n",
    "\n",
    "#结束时间\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "#计算时差\n",
    "minium = end_time - start_time\n",
    "\n",
    "print(\"执行了：\" + str(int(minium / 60)) + \"分\" + str(int(minium%60)) + \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4eec0",
   "metadata": {},
   "source": [
    "## 4 - 写出莎士比亚风格的文字（选学）\n",
    "\n",
    "这部分是可选的，类似的（但更复杂的）任务是产生莎士比亚诗歌。不用学习恐龙名字的数据集，你可以使用莎士比亚诗集。使用LSTM单元，我们可以学习跨越文本中许多字符的较长时间的依赖关系，例如，出现在某个序列的某个字符会影响在该序列后面的不同字符。由于恐龙名字很短，这些长期的依赖性与恐龙名字并不那么重要。我们用Keras实现了莎士比亚诗歌生成器，我们先来加载所需的包和模型，这可能需要几分钟\n",
    "![](images/shakespeare.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67cf68cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n",
      "执行了：0分2秒\n"
     ]
    }
   ],
   "source": [
    "#开始时间\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io\n",
    "\n",
    "#结束时间\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "#计算时差\n",
    "minium = end_time - start_time\n",
    "\n",
    "print(\"执行了：\" + str(int(minium / 60)) + \"分\" + str(int(minium%60)) + \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e235c",
   "metadata": {},
   "source": [
    "为了节省时间，我们已经为莎士比亚诗集《十四行诗》模型训练了1000代，让我们再训练一下这个模型。当它完成了一代的训练——这也需要几分钟——你可以运行generate_output，它会提示你输入(小于40个字符)。这首诗将从你的句子开始，我们的RNN-Shakespeare将为你完成这首诗的其余部分!例如，是试着输入“Forsooth this maketh no sense”(不要输入引号)。取决于最后是否包含空格，您的结果也可能有所不同——您可以同时尝试这两种方式，也可以尝试其他输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "766be106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 27s 103ms/step - loss: 2.5609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x269c9272ee0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "478299c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: hello\n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "helloned,\n",
      "debe onged yet ustey sak diks a kear,\n",
      "shate con there bose thiny da and hall, be'n wass in vinge,\n",
      "by herings, this and belurots from this voy,\n",
      "who gro bosr yew hid keaved, me hate dingut,\n",
      "eyef to say i am hos sungles u remint to dither,\n",
      "hade ans bitots saff) failss with thy you gone's be.\n",
      " \n",
      "\n",
      "s\n",
      "chathing a live which the refalles mude that being,\n",
      "thought my ells in that be you dims whose to bow"
     ]
    }
   ],
   "source": [
    "# 运行此代码尝试不同的输入，而不必重新训练模型。\n",
    "generate_output() #博主在这里输入hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c89c11d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"221pt\" height=\"644pt\" viewBox=\"0.00 0.00 166.00 483.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1.33333 1.33333) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-479 162,-479 162,4 -4,4\"/>\n",
       "<!-- 2653293854384 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2653293854384</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"11,-438.5 11,-474.5 147,-474.5 147,-438.5 11,-438.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"40.5\" y=\"-452.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input_3</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"70,-438.5 70,-474.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"108.5\" y=\"-452.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">InputLayer</text>\n",
       "</g>\n",
       "<!-- 2653292668864 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2653292668864</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"25,-365.5 25,-401.5 133,-401.5 133,-365.5 25,-365.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"52\" y=\"-379.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lstm_5</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"79,-365.5 79,-401.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"106\" y=\"-379.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">LSTM</text>\n",
       "</g>\n",
       "<!-- 2653293854384&#45;&gt;2653292668864 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2653293854384-&gt;2653292668864</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79,-438.31C79,-430.29 79,-420.55 79,-411.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"82.5,-411.53 79,-401.53 75.5,-411.53 82.5,-411.53\"/>\n",
       "</g>\n",
       "<!-- 2653293782928 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2653293782928</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"9,-292.5 9,-328.5 149,-328.5 149,-292.5 9,-292.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-306.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dropout_3</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"85,-292.5 85,-328.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-306.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dropout</text>\n",
       "</g>\n",
       "<!-- 2653292668864&#45;&gt;2653293782928 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2653292668864-&gt;2653293782928</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79,-365.31C79,-357.29 79,-347.55 79,-338.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"82.5,-338.53 79,-328.53 75.5,-338.53 82.5,-338.53\"/>\n",
       "</g>\n",
       "<!-- 2653293850832 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2653293850832</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"25,-219.5 25,-255.5 133,-255.5 133,-219.5 25,-219.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"52\" y=\"-233.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lstm_6</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"79,-219.5 79,-255.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"106\" y=\"-233.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">LSTM</text>\n",
       "</g>\n",
       "<!-- 2653293782928&#45;&gt;2653293850832 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2653293782928-&gt;2653293850832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79,-292.31C79,-284.29 79,-274.55 79,-265.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"82.5,-265.53 79,-255.53 75.5,-265.53 82.5,-265.53\"/>\n",
       "</g>\n",
       "<!-- 2653293629552 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2653293629552</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"9,-146.5 9,-182.5 149,-182.5 149,-146.5 9,-146.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-160.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dropout_4</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"85,-146.5 85,-182.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-160.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dropout</text>\n",
       "</g>\n",
       "<!-- 2653293850832&#45;&gt;2653293629552 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2653293850832-&gt;2653293629552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79,-219.31C79,-211.29 79,-201.55 79,-192.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"82.5,-192.53 79,-182.53 75.5,-192.53 82.5,-192.53\"/>\n",
       "</g>\n",
       "<!-- 2653293633488 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2653293633488</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"22,-73.5 22,-109.5 136,-109.5 136,-73.5 22,-73.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-87.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_3</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"85,-73.5 85,-109.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"110.5\" y=\"-87.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n",
       "</g>\n",
       "<!-- 2653293629552&#45;&gt;2653293633488 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2653293629552-&gt;2653293633488</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79,-146.31C79,-138.29 79,-128.55 79,-119.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"82.5,-119.53 79,-109.53 75.5,-119.53 82.5,-119.53\"/>\n",
       "</g>\n",
       "<!-- 2653293633008 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2653293633008</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-36.5 158,-36.5 158,-0.5 0,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"42\" y=\"-14.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">activation_3</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"84,-0.5 84,-36.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"121\" y=\"-14.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Activation</text>\n",
       "</g>\n",
       "<!-- 2653293633488&#45;&gt;2653293633008 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2653293633488-&gt;2653293633008</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79,-73.31C79,-65.29 79,-55.55 79,-46.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"82.5,-46.53 79,-36.53 75.5,-46.53 82.5,-46.53\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------用于绘制模型细节，可选--------------#\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "%matplotlib inline\n",
    "plot_model(model, to_file='shakespeare.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "#------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573714d",
   "metadata": {},
   "source": [
    "RNN莎士比亚模型与你为恐龙名字建立的模型非常相似。唯一的主要区别是：\n",
    "- LSTM代替基本的RNN捕获更长的距离依赖关系\n",
    "- 该模型是一个更深层的LSTM模型（2层）\n",
    "- 使用keras而不是Python来简化代码"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
